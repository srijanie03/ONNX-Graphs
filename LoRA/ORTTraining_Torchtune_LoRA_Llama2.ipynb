{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b762148-90ce-462b-ae64-f8d1b090c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.models.llama2 import llama2_7b, lora_llama2_7b\n",
    "import onnxruntime.training.onnxblock as onnxblock\n",
    "from onnxruntime.training.api import CheckpointState, Module, Optimizer\n",
    "from onnxruntime.training import artifacts\n",
    "from onnxruntime import InferenceSession\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import onnx\n",
    "import io\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059dbcb2-c8d9-4871-81b2-00d8a1d69625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Llama2 without any LoRA layers\n",
    "base_model = llama2_7b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426da459-634f-41aa-9a89-d2d32dff95e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default settings for lora_llama2_7b will match those for llama2_7b\n",
    "# We just need to define which layers we want LoRA applied to.\n",
    "# Within each self-attention, we can choose from [\"q_proj\", \"k_proj\", \"v_proj\", and \"output_proj\"].\n",
    "# We can also set apply_lora_to_mlp=True or apply_lora_to_output=True to apply LoRA to other linear\n",
    "# layers outside of the self-attention.\n",
    "lora_model = lora_llama2_7b(lora_attn_modules=[\"q_proj\", \"v_proj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb81aa-842f-4d08-a437-39dcae1604e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.tensor([[6109,3626,6100,345],[6109,1110,6622,257]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c26502-205b-4519-8a39-9d52e20b22e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward graph\n",
    "\n",
    "model_outputs = lora_model(batch)\n",
    "\n",
    "if isinstance(model_outputs, torch.Tensor):\n",
    "    model_outputs = [model_outputs]\n",
    "\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"output\"]\n",
    "dynamic_axes = {\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
    "\n",
    "\n",
    "f = io.BytesIO()\n",
    "torch.onnx.export(\n",
    "    lora_model,\n",
    "    batch,\n",
    "    \"torchtune_lora_llama2.onnx\",\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    opset_version=14,\n",
    "    do_constant_folding=False,\n",
    "    training=torch.onnx.TrainingMode.TRAINING,\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    export_params=True,\n",
    "    keep_initializers_as_inputs=False,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa75427-4754-4724-a3e7-1de016c0d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward graph\n",
    "\n",
    "requires_grad = [name for name, param in lora_model.named_parameters() if param.requires_grad]\n",
    "frozen_params = [name for name, param in lora_model.named_parameters() if not param.requires_grad]\n",
    "\n",
    "artifacts.generate_artifacts(\n",
    "    \"torchtune_lora_llama2.onnx\",\n",
    "    optimizer=artifacts.OptimType.AdamW,\n",
    "    loss=artifacts.LossType.CrossEntropyLoss, #Specify the loss function, try with different ones\n",
    "    #loss=artifacts.LossType.MSELoss,\n",
    "    requires_grad=requires_grad,\n",
    "    frozen_params=frozen_params,\n",
    "    artifact_directory=\"torchtune_lora_llama2\",\n",
    "    additional_output_names=[\"output\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
